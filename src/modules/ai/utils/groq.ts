import { logger } from '../../../utils/logger';

interface GenerateOptions {
  maxTokens?: number;
  temperature?: number;
  systemPrompt?: string;
  useCase?: 'chat' | 'moderation' | 'support' | 'dev'; // Ajout 'dev'
}

interface GroqErrorResponse {
  error?: {
    message?: string;
    code?: string;
    type?: string;
  };
}

export class GroqClient {
  private apiKey: string;
  private baseUrl = 'https://api.groq.com/openai/v1';
  
  // ‚ö° ARCHITECTURE HYBRIDE MULTI-MOD√àLES
  // Chat: llama-3.3-70b-versatile (30 RPM, 1000 RPD) -> fallback llama-3.1-8b-instant (30 RPM, 14400 RPD)
  // Moderation: llama-guard-3-8b (30 RPM, 14400 RPD) - Sp√©cialis√© s√©curit√©
  // Support: qwen-32b-instruct (30 RPM, 14400 RPD) - Expertise technique
  // Dev: openai/gpt-oss-120b (30 RPM, 1000 RPD) - Code & raisonnement avanc√©
  private chatPrimaryModel = 'llama-3.3-70b-versatile';
  private chatFallbackModel = 'llama-3.1-8b-instant';
  private moderationModel = 'llama-guard-3-8b';
  private supportModel = 'qwen-32b-instruct';
  private devModel = 'openai/gpt-oss-120b';

  constructor(apiKey: string) {
    if (!apiKey || apiKey === 'your_groq_api_key_here') {
      throw new Error('GROQ_API_KEY is not configured. Please set a valid API key in your .env file.');
    }
    this.apiKey = apiKey;
    logger.info(`üöÄ Groq client initialized with hybrid architecture:`);
    logger.info(`   - Chat: ${this.chatPrimaryModel} (fallback: ${this.chatFallbackModel})`);
    logger.info(`   - Moderation: ${this.moderationModel}`);
    logger.info(`   - Support: ${this.supportModel}`);
    logger.info(`   - Dev: ${this.devModel}`);
  }

  private selectModel(useCase?: string): string {
    switch (useCase) {
      case 'moderation':
        return this.moderationModel;
      case 'support':
        return this.supportModel;
      case 'dev':
        return this.devModel;
      case 'chat':
      default:
        return this.chatPrimaryModel;
    }
  }

  async generateText(prompt: string, options: GenerateOptions = {}): Promise<string> {
    const selectedModel = this.selectModel(options.useCase);
    
    try {
      return await this.executeRequest(selectedModel, prompt, options);
    } catch (error: any) {
      // Fallback uniquement pour le chat si erreur 429 (rate limit)
      if (options.useCase === 'chat' && error.message && error.message.includes('429')) {
        logger.warn(`‚ö†Ô∏è Rate limit atteint sur ${this.chatPrimaryModel}, fallback vers ${this.chatFallbackModel}`);
        try {
          return await this.executeRequest(this.chatFallbackModel, prompt, options);
        } catch (fallbackError: any) {
          logger.error('Fallback model also failed:', fallbackError);
          throw fallbackError;
        }
      }
      throw error;
    }
  }

  private async executeRequest(model: string, prompt: string, options: GenerateOptions): Promise<string> {
    try {
      const url = `${this.baseUrl}/chat/completions`;
      
      logger.debug('Groq API request:', {
        model: model,
        useCase: options.useCase || 'default',
        promptLength: prompt.length,
      });

      const messages: any[] = [];
      
      if (options.systemPrompt) {
        messages.push({
          role: 'system',
          content: options.systemPrompt,
        });
      }
      
      messages.push({
        role: 'user',
        content: prompt,
      });

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: model,
          messages,
          max_tokens: options.maxTokens || 8192,
          temperature: options.temperature !== undefined ? options.temperature : 1.0,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({})) as GroqErrorResponse;
        
        logger.error('Groq API error details:', {
          status: response.status,
          statusText: response.statusText,
          error: errorData,
          model: model,
          apiKeyPrefix: this.apiKey.substring(0, 12),
        });
        
        let errorMessage = '';
        
        if (response.status === 400) {
          const details = errorData?.error?.message || JSON.stringify(errorData);
          errorMessage = `‚ùå Requ√™te invalide (400): ${details}\n‚ÑπÔ∏è V√©rifiez votre GROQ_API_KEY dans .env`;
        } else if (response.status === 401) {
          errorMessage = `‚ùå API Key invalide (401)\n‚ÑπÔ∏è Cr√©ez une nouvelle cl√© sur https://console.groq.com/keys`;
        } else if (response.status === 429) {
          const details = errorData?.error?.message || 'Trop de requ√™tes';
          errorMessage = `‚è≥ Limite de requ√™tes atteinte (429) - Mod√®le: ${model}\n${details}\n‚ÑπÔ∏è Attendez 60 secondes ou v√©rifiez votre quota`;
        } else if (response.status === 500 || response.status === 503) {
          errorMessage = `‚ö†Ô∏è Erreur serveur Groq (${response.status})\n‚ÑπÔ∏è R√©essayez dans quelques instants`;
        } else {
          const details = errorData?.error?.message || response.statusText;
          errorMessage = `‚ùå Erreur API Groq (${response.status}): ${details}`;
        }
        
        throw new Error(errorMessage);
      }

      const data = await response.json() as any;
      
      logger.debug('Groq API response:', {
        model: model,
        hasChoices: !!data.choices,
        choicesCount: data.choices?.length || 0,
      });
      
      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        logger.error('Groq response structure invalid:', JSON.stringify(data));
        throw new Error('‚ùå R√©ponse API Groq invalide (pas de contenu)\n‚ÑπÔ∏è La r√©ponse ne contient pas de texte g√©n√©r√©');
      }
      
      return data.choices[0].message.content;
    } catch (error: any) {
      if (error.message && error.message.includes('‚ùå')) {
        throw error;
      }
      
      logger.error('Groq API unexpected error:', error);
      throw new Error(`‚ùå Erreur inattendue: ${error.message || "Impossible de contacter l'API Groq"}`);
    }
  }

  async analyzeToxicity(text: string): Promise<number> {
    try {
      // Utilise Llama Guard 3 8B sp√©cialis√© pour la mod√©ration
      const response = await this.generateText(
        `Analyze the following message for toxicity, harassment, hate speech, or inappropriate content. Respond ONLY with a number between 0.0 and 1.0, where 0.0 is completely safe and 1.0 is extremely toxic.\n\nMessage: "${text}"\n\nToxicity score:`,
        {
          maxTokens: 10,
          temperature: 0.1,
          systemPrompt: 'You are a toxicity analyzer. Respond ONLY with a decimal number between 0.0 and 1.0.',
          useCase: 'moderation', // Utilise llama-guard-3-8b
        }
      );

      const score = parseFloat(response.trim());
      return isNaN(score) ? 0 : Math.max(0, Math.min(1, score));
    } catch (error) {
      logger.error('Toxicity analysis error:', error);
      return 0; // Default to safe if error
    }
  }
}
